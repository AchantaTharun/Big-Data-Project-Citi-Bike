{"cells": [{"cell_type": "markdown", "id": "47d629e6-ea87-4936-a699-368413ce127d", "metadata": {}, "source": "# Importing libraries and initializing spark session"}, {"cell_type": "markdown", "id": "63d46963-99dd-4c5a-8044-5a8d3205c980", "metadata": {"tags": []}, "source": "### Creating spark session and reading data from previous pipeline"}, {"cell_type": "code", "execution_count": 1, "id": "1fea328e-a3d1-4a6e-a999-f7f597eab90b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/12/21 21:27:11 INFO SparkEnv: Registering MapOutputTracker\n24/12/21 21:27:11 INFO SparkEnv: Registering BlockManagerMaster\n24/12/21 21:27:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/21 21:27:11 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nimport os\n\nspark =( SparkSession.builder\n    .appName(\"Models\")\n    .config(\"spark.sql.shuffle.partitions\", 20)\n    .getOrCreate())\n\n\ntrain = \"gs://bucket121024/pipeline2/train_data.parquet\"\ntest = \"gs://bucket121024/pipeline2/test_data.parquet\""}, {"cell_type": "markdown", "id": "88fa61d4-a145-4bb0-ba29-62e3eee491a3", "metadata": {}, "source": "# Reading citibike combined data"}, {"cell_type": "markdown", "id": "60d2a9a7-1147-4651-ba5e-7f514f21b207", "metadata": {}, "source": "### Efficient Parallelism:\n\n### 80 partitions for training data distribute the workload evenly across 20 cores.\n### 40 partitions for testing data minimize task scheduling overhead."}, {"cell_type": "code", "execution_count": 2, "id": "2827c6f6-f7e3-4224-af23-0416fe609108", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "\ntrain_data = spark.read.parquet(train)\ntest_data = spark.read.parquet(test)\ntrain_data = train_data.sample(withReplacement=False, fraction=0.6)\ntest_data = test_data.sample(withReplacement=False, fraction=0.6)\ntrain_data = train_data.repartition(80)\ntest_data = test_data.repartition(40)"}, {"cell_type": "code", "execution_count": 3, "id": "a6b0e3cf-f7d7-4dbc-80ca-7a05399500a0", "metadata": {"tags": []}, "outputs": [], "source": "feature_cols = [\"day_of_week\", \"month\", \"hour\", \"year\", \"temp\", \"humidity\", \n                \"precip\",  \"windspeed\", \"visibility\", \n                 \"is_weekend\", \"is_lockdown\",\"startstationname_indexed\", \"humidity_is_weekend\", \"temp_is_weekend\",\"hour_bucket_indexed\", \"rolling_avg_demand\", \"lag_demand_1\" ]"}, {"cell_type": "markdown", "id": "2e806e97-d10e-45a0-b3fd-0c8a97cff4b7", "metadata": {}, "source": "* `VectorAssembler`\nCombines multiple input features into a single vector column (features), required for ML models.\n* `StandardScaler`\nStandardizes the feature vector by removing the mean and scaling to unit variance."}, {"cell_type": "code", "execution_count": 4, "id": "6b392099-350a-4454-9d1a-c11d29d1246a", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml import Pipeline\n\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n"}, {"cell_type": "code", "execution_count": 5, "id": "d04e719a-3d86-43df-881c-565e78d946c6", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.tuning import TrainValidationSplit\n\ndt = DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=\"demand\")\n\n#pipeline\npipeline = Pipeline(stages=[assembler, scaler, dt])"}, {"cell_type": "markdown", "id": "618bb6c1-f5ca-4fcc-950d-05dd2be284db", "metadata": {"tags": []}, "source": "### Hyperparameter Tuning with `TrainValidationSplit`\n\nTo optimize the performance of the Decision Tree Regressor, we implemented hyperparameter tuning using `TrainValidationSplit`.\n\n#### **1. Define the Parameter Grid**\nWe specify the hyperparameters to tune:\n- **`maxDepth`:** Maximum depth of the tree (values: 3, 5).\n- **`minInstancesPerNode`:** Minimum number of samples per tree node (values: 1, 2).\n- **`maxBins`:** Number of bins for continuous feature discretization (values: 16, 32).\n\n"}, {"cell_type": "code", "execution_count": 6, "id": "f3e837df-e1f0-44e7-857e-abb89fabf380", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "DataFrame[date: date, day_of_week: int, month: int, hour: int, year: int, startstationname: string, demand: bigint, datetime: timestamp, temp: double, humidity: double, precip: double, windspeed: double, visibility: double, conditions: string, is_weekend: int, is_lockdown: int, conditions_indexed: double, startstationname_indexed: double, lag_demand_1: bigint, rolling_avg_demand: double, hour_bucket: string, hour_bucket_indexed: double, temp_is_weekend: double, humidity_is_weekend: double]"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "train_data.cache()"}, {"cell_type": "code", "execution_count": 7, "id": "2a8057de-c384-4955-9107-0fdb2aaf734b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/21 19:41:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 158:====================================================>  (38 + 2) / 40]\r"}, {"name": "stdout", "output_type": "stream", "text": "Root Mean Squared Error (RMSE): 2.3371475155200025\nR-Squared (R2): 0.7243980320844088\nMean Absolute Error (MAE): 1.5108403626289202\nMean Squared Error (MSE): 5.462258509301319\nCPU times: user 766 ms, sys: 218 ms, total: 983 ms\nWall time: 6min 11s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time\n\nparam_grid = (ParamGridBuilder()\n              .addGrid(dt.maxDepth, [3, 5])  \n              .addGrid(dt.minInstancesPerNode, [1, 2]) \n              .addGrid(dt.maxBins, [16])\n              .build())\nevaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n\ntrain_val_split = TrainValidationSplit(estimator=pipeline,\n                                       estimatorParamMaps=param_grid,\n                                       evaluator=evaluator,\n                                       trainRatio=0.8)\n\n\ntv_model = train_val_split.fit(train_data)\n\nbest_model = tv_model.bestModel\npredictions = best_model.transform(test_data)\n\n\nrmse_evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = rmse_evaluator.evaluate(predictions)\n\nr2_evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = r2_evaluator.evaluate(predictions)\n\nmae_evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mae\")\nmae = mae_evaluator.evaluate(predictions)\n\nmse_evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mse\")\nmse = mse_evaluator.evaluate(predictions)\n\n# Print all metrics\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\nprint(f\"R-Squared (R2): {r2}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\n"}, {"cell_type": "code", "execution_count": 8, "id": "98a2bc27-669d-4ef3-8b75-223ae06c94b5", "metadata": {"tags": []}, "outputs": [], "source": "#predictions.write.mode(\"overwrite\").parquet(\"gs://bucket121024/pipeline3/5_DT.parquet\")"}, {"cell_type": "markdown", "id": "5f3432f4-eba6-48e1-9f9d-beb540129455", "metadata": {"tags": []}, "source": "# Random Forest Regressor\n## Hyperparameter Tuning with `ParamGridBuilder`\n\nThe `ParamGridBuilder` is used to define a grid of hyperparameters for tuning the Random Forest Regressor. In this case:\n\n### Explanation of Parameters:\n1. **`numTrees`**:\n   - The number of decision trees in the Random Forest.\n   - Tested values: `50`, `100`.\n\n2. **`maxDepth`**:\n   - The maximum depth of each tree, which controls the complexity of the model.\n   - Tested values: `5`, `10`.\n\n### Total Combinations:\nThe total combinations of hyperparameters are:\n\\[\n\\text{Total Combinations} = \\text{len(numTrees)} \\times \\text{len(maxDepth)} = 2 \\times 2 = 4\n\\]\n\nEach combination is evaluated during training to select the best-performing model based on validation metrics.\n"}, {"cell_type": "code", "execution_count": 9, "id": "a0edd3e4-442b-4cca-867a-334d8742d34a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "WARNING: An illegal reflective access operation has occurred                    \nWARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.1.jar) to field java.nio.charset.Charset.name\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n24/12/21 19:52:27 WARN DAGScheduler: Broadcasting large task binary with size 1180.8 KiB\n24/12/21 19:53:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n24/12/21 19:54:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n24/12/21 19:55:44 WARN DAGScheduler: Broadcasting large task binary with size 1231.1 KiB\n24/12/21 19:55:47 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n24/12/21 19:57:08 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n24/12/21 20:05:59 WARN DAGScheduler: Broadcasting large task binary with size 1170.3 KiB\n24/12/21 20:07:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n24/12/21 20:09:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n24/12/21 20:11:52 WARN DAGScheduler: Broadcasting large task binary with size 1237.2 KiB\n24/12/21 20:11:55 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n24/12/21 20:14:27 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n24/12/21 20:14:33 WARN DAGScheduler: Broadcasting large task binary with size 15.8 MiB\n24/12/21 20:17:38 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n24/12/21 20:23:39 WARN DAGScheduler: Broadcasting large task binary with size 1157.5 KiB\n24/12/21 20:25:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n24/12/21 20:27:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n24/12/21 20:31:05 WARN DAGScheduler: Broadcasting large task binary with size 1237.4 KiB\n24/12/21 20:31:08 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n24/12/21 20:34:22 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n24/12/21 20:34:27 WARN DAGScheduler: Broadcasting large task binary with size 15.8 MiB\n24/12/21 20:38:16 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n[Stage 370:=====================================================> (39 + 1) / 40]\r"}, {"name": "stdout", "output_type": "stream", "text": "Random Forest Regressor - RMSE: 2.1155644320309652\nRandom Forest Regressor - R2: 0.7741799090215069\nRandom Forest Regressor - MAE: 1.3778026324541734\nRandom Forest Regressor - MSE: 4.475612866074501\nCPU times: user 2.84 s, sys: 623 ms, total: 3.47 s\nWall time: 54min 7s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=\"demand\")\n\nrf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n\nparam_grid = (ParamGridBuilder()\n              .addGrid(rf.numTrees, [50, 100])  # Tune number of trees\n              .addGrid(rf.maxDepth, [5, 10])    # Tune maximum depth of trees\n              .build())\n\nevaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n\ntrain_val_split = TrainValidationSplit(estimator=rf_pipeline,\n                                       estimatorParamMaps=param_grid,\n                                       evaluator=evaluator,\n                                       trainRatio=0.8)\n\nrf_model = train_val_split.fit(train_data)\n\n# Get the best model\nbest_rf_model = rf_model.bestModel\n\nrf_predictions = best_rf_model.transform(test_data)\n\n# Evaluate the model\nrf_rmse = evaluator.evaluate(rf_predictions)  # RMSE\nrf_r2 = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(rf_predictions)  # R2\nrf_mae = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(rf_predictions)  # MAE\nrf_mse = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mse\").evaluate(rf_predictions)  # MSE\n\n# Print metrics\nprint(f\"Random Forest Regressor - RMSE: {rf_rmse}\")\nprint(f\"Random Forest Regressor - R2: {rf_r2}\")\nprint(f\"Random Forest Regressor - MAE: {rf_mae}\")\nprint(f\"Random Forest Regressor - MSE: {rf_mse}\")\n\n"}, {"cell_type": "code", "execution_count": 10, "id": "07237ad2-a05c-4ecd-975c-275bee40a46f", "metadata": {"tags": []}, "outputs": [], "source": "#rf_predictions.write.mode(\"overwrite\").parquet(\"gs://bucket121024/pipeline3/5_RF.parquet\")"}, {"cell_type": "markdown", "id": "2e3b75d8-1b11-45de-b1c2-8ff3797614f6", "metadata": {}, "source": "- **`regParam`**: Regularization parameter (lambda) to prevent overfitting.\n  - Values tested: `[0.1, 0.3]`\n- **`elasticNetParam`**: Mixing parameter to balance L1 (Lasso) and L2 (Ridge) regularization.\n  - Values tested: `[0.0, 0.5]`\n  - `0.0`: Pure L2 (Ridge) regularization.\n  - `0.5`: A mix of L1 and L2 regularization."}, {"cell_type": "code", "execution_count": 11, "id": "7e3a2a4d-c115-4366-b604-41cb837496d7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 457:====================================================>  (38 + 2) / 40]\r"}, {"name": "stdout", "output_type": "stream", "text": "Linear Regression - Best RMSE: 2.440002208602954\nLinear Regression - Best R2: 0.6996065192039672\nLinear Regression - Best MAE: 1.6498225305436467\nLinear Regression - Best MSE: 5.9536107779872935\nCPU times: user 525 ms, sys: 161 ms, total: 685 ms\nWall time: 3min 31s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time\n\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"demand\")\n\nlr_pipeline = Pipeline(stages=[assembler, scaler, lr])\n\n\nparam_grid = (ParamGridBuilder()\n              .addGrid(lr.regParam, [0.1, 0.3])\n              .addGrid(lr.elasticNetParam, [0.0, 0.5])\n              .build())\n\n\nrmse_evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n\n\ntrain_val_split = TrainValidationSplit(estimator=lr_pipeline,\n                                       estimatorParamMaps=param_grid,\n                                       evaluator=rmse_evaluator,\n                                       trainRatio=0.8)\n\n\nlr_model = train_val_split.fit(train_data)\n\nbest_lr_model = lr_model.bestModel\n\nlr_predictions = best_lr_model.transform(test_data)\n\nlr_rmse = rmse_evaluator.evaluate(lr_predictions)  # RMSE\nlr_r2 = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(lr_predictions)  # R\u00b2\nlr_mae = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(lr_predictions)  # MAE\nlr_mse = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mse\").evaluate(lr_predictions)  # MSE\n\n# Print all evaluation metrics\nprint(f\"Linear Regression - Best RMSE: {lr_rmse}\")\nprint(f\"Linear Regression - Best R2: {lr_r2}\")\nprint(f\"Linear Regression - Best MAE: {lr_mae}\")\nprint(f\"Linear Regression - Best MSE: {lr_mse}\")\n"}, {"cell_type": "code", "execution_count": 12, "id": "e4c29770-8c44-4986-ad01-06cff3244a96", "metadata": {"tags": []}, "outputs": [], "source": "#lr_predictions.write.mode(\"overwrite\").parquet(\"gs://bucket121024/pipeline3/5_LR.parquet\")"}, {"cell_type": "markdown", "id": "976578c0-1dc6-4016-9625-dc5c31e9a857", "metadata": {"tags": []}, "source": "### Hyperparameter Tuning for Gradient Boosted Trees (GBT)\n\nTo optimize the performance of the Gradient Boosted Trees (GBT) model, a minimal hyperparameter tuning process was implemented using the following key parameters:\n\n1. **`maxDepth`**:\n   - Controls the maximum depth of the trees.\n   - A deeper tree can model more complex relationships but risks overfitting.\n   - Values tested: `[3, 5]`.\n\n2. **`maxIter`**:\n   - Determines the number of boosting iterations.\n   - More iterations allow the model to refine predictions but increase computation time.\n   - Values tested: `[10, 20]`.\n\n3. **`stepSize`**:\n   - Represents the learning rate for gradient boosting.\n   - Smaller step sizes make the model converge more slowly but can improve generalization.\n   - Values tested: `[0.05, 0.1]`.\n\nThe chosen hyperparameter ranges strike a balance between computational efficiency and model performance, ensuring scalability for large datasets.\n"}, {"cell_type": "code", "execution_count": 7, "id": "ec0be690-b7ff-417b-8162-ed8a7726ef58", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/21 21:27:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\nWARNING: An illegal reflective access operation has occurred                    \nWARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.1.jar) to field java.nio.charset.Charset.name\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n[Stage 1895:===================================================>  (38 + 2) / 40]\r"}, {"name": "stdout", "output_type": "stream", "text": "GBT Regressor - Best RMSE: 2.0771982051303164\nGBT Regressor - Best R2: 0.7819135375634417\nGBT Regressor - Best MAE: 1.3487031977958273\nGBT Regressor - Best MSE: 4.314752383396607\nCPU times: user 3.13 s, sys: 669 ms, total: 3.8 s\nWall time: 22min 29s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time\n\nfrom pyspark.ml.regression import GBTRegressor\n\n# Initialize GBT Regressor\ngbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=\"demand\")\n\ngbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])\n\n# hyperparameter grid for tuning\nparam_grid = (ParamGridBuilder()\n              .addGrid(gbt.maxDepth, [3, 5]) \n              .addGrid(gbt.maxIter, [10, 20])\n              .addGrid(gbt.stepSize, [0.05, 0.1])\n              .build())\n\nevaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\ntrain_val_split = TrainValidationSplit(estimator=gbt_pipeline,\n                                       estimatorParamMaps=param_grid,\n                                       evaluator=evaluator,\n                                       trainRatio=0.8)\ngbt_model = train_val_split.fit(train_data)\nbest_gbt_model = gbt_model.bestModel\n\ngbt_predictions = best_gbt_model.transform(test_data)\n\ngbt_rmse = evaluator.evaluate(gbt_predictions)  # RMSE\ngbt_r2 = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(gbt_predictions)  # R\u00b2\ngbt_mae = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(gbt_predictions)  # MAE\ngbt_mse = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"mse\").evaluate(gbt_predictions)  # MSE\n\n# Print evaluation metrics\nprint(f\"GBT Regressor - Best RMSE: {gbt_rmse}\")\nprint(f\"GBT Regressor - Best R2: {gbt_r2}\")\nprint(f\"GBT Regressor - Best MAE: {gbt_mae}\")\nprint(f\"GBT Regressor - Best MSE: {gbt_mse}\")\n"}, {"cell_type": "code", "execution_count": null, "id": "e508338d-7f41-437c-aa38-e9e7eafe6041", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}